<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Assistant Professor of Computer Vision</h2>
            </header>

                <p>I am a lecturer/Assistant Professor of Computer Vision at
                the Department of Computer Science at the University of
                Bristol. My research interests are in multi-modal video
                understanding, particularly for egocentric videos â€” focusing on
                how both vision and language can be tied together towards tasks
                such as cross-modal retrieval, grounding and captioning.
                I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
                <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://twitter.com/mwray0">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">January 2023</span> - <b>EPIC-Kitchens 2023 Challenges Open!</b> - The challenges for the 2023 versions of the EPIC-Kitchens Challenges are now open. Find out more <a href="https://epic-kitchens.github.io/2023">here</a>. </li>
                <li> <span class="prim-colour">January 2023</span> - <b>Organiser of Ego4D & EPIC workshop at CVPR2023</b> - I am an organiser for the 3rd Ego4D and 11th EPIC Workshop at CVPR2023. </li>
                <li> <span class="prim-colour">November 2022</span> - <b>Area Chair at CVPR 2023</b> - Honoured to serve as an area chair for CVPR 2023.</li>
                <li> <span class="prim-colour">October 2022</span> - <b>Outstanding Reviewer at ECCV2022</b>
                <li> <span class="prim-colour">October 2022</span> - <b>Paper Accepted at ACCV 2022</b> - Our paper titled "ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval" was accepted at ACCV 2022 as an Oral. See more <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">here</a>.
                <li> <span class="prim-colour">September 2022</span> - <b>Paper Accepted at NeurIPS 2022</b> - Our paper titled "Egocentric Video-Language Pretraining" was accepted at NeurIPS 2022. See more <a href="https://qinghonglin.github.io/EgoVLP/">here</a>.
                <li> <span class="prim-colour">September 2022</span> - <b>Talk at Video Understanding Symposium</b> - I presented a talk about Ego4D and verbs in video understanding during the second Video Understanding Symposium at the University of Amsterdam.
                <li> <span class="prim-colour">August 2022</span> - <b>Call for Extended Abstracts Ego4D@ECCV2022</b> - You can submit extended abstracts of ongoing/published work at the 2nd Int'l Ego4D workshop at ECCV2022. Full details <a href="https://ego4d-data.org/workshops/eccv22/">here</a>.
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of Research projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/contra_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval</span>
                            <br>
                            Adriano Fragomeni, <span>Michael Wray</span>, Dima
                            Damen,
                            <br>
                            <em>ACCV (Oral), 2022</em>
                            <br>
                            <strong>
                                <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2210.04341">[arXiv]</a>
                                <a href="https://github.com/adrianofragomeni/ConTra">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/egovlp.jpeg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Egocentric Video-Language Pretraining</span>
                            <br>
                            Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia
                            Soldan, <span>Michael Wray</span>, Rui Yan, Eric
                            Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao,
                            Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen,
                            Bernard Ghanem, Wei Liu, Mike Zheng Shou
                            <br>
                            <em>NeurIPS, 2022</em>
                            <br>
                            <strong>
                                <a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2206.01670">[arXiv]</a>
                                <a href="https://github.com/showlab/EgoVLP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ego4d.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego4D: Around the World in 3,000 Hours of Egocentric Video</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, <span>Michael Wray</span>, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR) 2022</em>
                            <br>
                            <strong>
                                <a href="https://ego4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2110.07058">[arXiv]</a>
                                <a href="https://ego4d-data.org/docs/">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/DACMR.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval</span>
                            <br>
                            Jonathan Munro, <span>Michael Wray</span>, Diane
                            Larlus, Gabriela Csurka, Dima Damen
                            <br>
                            <em>ArXiv, 2021</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2110.12812">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/epic_kitchens.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Rescaling Egocentric Vision</span>
                            <br>
                            Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and <span>Michael Wray</span>
                            <br>
                            <em>Springer International Journal of Computer Vision (IJCV), 2021</em>
                            <br>
                            <strong>
                                <a href="https://epic-kitchens.github.io/">[Webpage]</a>
                                <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ssvr_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">On Semantic Similarity in Video Retrieval</span>
                            <br>
                            <span>Michael Wray</span>, Hazel Doughty, and Dima Damen
                            <br>
                            <em>IEEE Conference on Computer Vision and Pattern Recognition, 2021.</em>
                            <br>
                            <strong>
                                <a href="https://mwray.github.io/SSVR/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2103.10095">[arXiv]</a>
                                <a href="https://github.com/mwray/Semantic-Video-Retrieval">[Code]</a>
                                <a href="https://mwray.github.io/SSVR/SSVR.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3>Short Bio</h3>

            Michael is a lecturer in Computer Vision at the Computer Science
            Department at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li><span class="prim-colour">Applied Deep Learning</span> 2022/2023. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                <li><span class="prim-colour">Computer Systems A</span> 2022/2023. <a href="https://uob-csa.github.io/website/">Webpage</a>.
            </ul>

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current (w/ Dima Damen)</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current (w/ Dima Damen)</li>
                <li><span class="prim-colour">Shijia Feng</a></span>: PhD, 2022&#8211;Current (w/ Walterio Mayol Cuevas)</li>
            </ul>

            <h4>Previous</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
            </ul>

            <hr>

            <h3>Misc.</h3>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
                <li><span class="prim-colour">Ego4D@</span>: <a href="https://ego4d-data.org/workshops/eccv22/">ECCV2022</a></li>
            </ul>

            <h4>Area Chair</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR2023</span></li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">ECCV2022</span></li>
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>


            <h4> Reviewing Duties</h4>

            <h5> Conferences</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">ICCV: </span>2021</li>
                <li><span class="prim-colour">ECCV: </span>2022</li>
                <li><span class="prim-colour">ACCV: </span>2022, 2020</li>
                <li><span class="prim-colour">BMVC: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">WACV: </span>2022, 2021</li>
            </ul>
            
            <h5> Journals</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">TPAMI</span></li>
                <li><span class="prim-colour">IJCV</span></li>
                <li><span class="prim-colour">TCSVT</span></li>
                <li><span class="prim-colour">Pattern Recognition</span></li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

