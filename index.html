<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Assistant Professor of Computer Vision</h2>
            </header>

                <p>I am a lecturer/Assistant Professor of Computer Vision at
                the School of Computer Science at the University of
                Bristol. My research interests are in multi-modal video
                understanding, particularly for egocentric videos — focusing on
                how both vision and language can be tied together towards tasks
                such as cross-modal retrieval, grounding and captioning.
                I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
                <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://twitter.com/mwray0">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">February 2025</span> - <b>New Paper on ArXiv</b> Our paper: "Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval" is now on ArXiv, <a href="https://keflanagan.github.io/Moment-of-Untruth/">webpage</a>.
                <li> <span class="prim-colour">February 2025</span> - <b>New Paper on ArXiv</b> Our paper: "HD-EPIC: A Highly-Detailed Egocentric Video Dataset" is now on ArXiv, <a href="https://hd-epic.github.io/">webpage</a>.
                <li> <span class="prim-colour">December 2024</span> - <b>New Paper on ArXiv</b> Our paper: "ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions" is now on ArXiv, <a href="https://soczech.github.io/showhowto/">webpage</a>.
                <li> <span class="prim-colour">December 2024</span> - <b>Invited Talk at Durham</b> I gave a talk on Fine Grained Video Understanding from a Personal Perspective.
                <li> <span class="prim-colour">December 2024</span> - <b>External Examiner</b> Honoured to have been an external examiner for Tianqiao Chen.
                <li> <span class="prim-colour">October 2024</span> - <b>Paper Accepted at WACV 2025</b> Our paper titled Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval was accepted at WACV 2025. More details coming soon!
                <li> <span class="prim-colour">September 2024</span> - <b>Area Chair at CVPR 2025</b> Honoured to be an Area Chair for CVPR 2025.
                <li> <span class="prim-colour">August 2024</span> - <b>UCL Workshop Talk</b> I gave a talk on Egocentric Video Understanding at Advancements in Time Series Analysis for Computer Vision.
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of recent Research Projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/hd-epic_intro_fig.jpg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HD-EPIC: A Highly Detailed Egocentric Dataset</span>
                            <br>
                            Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, <span>Michael Wray</span>, Hazel Doughty, Dima Damen                            <br>
                            <em>arXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://hd-epic.github.io/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2502.04144">[arXiv]</a>
                                <a href="https://github.com/hd-epic/hd-epic-annotations">[Annotations]</a>
                                <a href="https://uob-my.sharepoint.com/:f:/g/personal/xy23932_bristol_ac_uk/Er39VjjlqKJFkNXp_tYdDaYBKEPnxNSz9GQ-VvXFW-yWWQ?e=555IHx">[Videos]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/untruth_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval</span>
                            <br>
                            Kevin Flanagan, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>WACV, 2025</em>
                            <br>
                            <strong>
                                <a href="https://keflanagan.github.io/Moment-of-Untruth/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2502.08544">[arXiv]</a>
                                <a href="https://github.com/keflanagan/MomentofUntruth">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/showhowto_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions</span>
                            <br>
                            Tomáš Souček, Prajwal Gatti, <span>Michael Wray</span>, Ivan Laptev, Dima Damen, Josef Sivic
                            <br>
                            <em>arXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://soczech.github.io/showhowto/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2412.01987">[arXiv]</a>
                                <a href="https://github.com/soCzech/showhowto">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/SHARP_qualitative.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</span>
                            <br>
                            Wiktor Mucha, <span>Michael Wray</span>, Martin Kampel
                            <br>
                            <em>ICPR, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2408.10037">[arXiv]</a>
                                <a href="https://github.com/wiktormucha/SHARP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/hoi-ref_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision</span>
                            <br>
                            Siddhant Bansal, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>arXiv, 2024</em>
                            <br>
                            <strong>
                                <a href="https://sid2697.github.io/hoi-ref/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2404.09933">[arXiv]</a>
                                <a href="https://github.com/Sid2697/HOI-Ref">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/video_edit_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Video Editing for Video Retrieval</span>
                            <br>
                            Bin Zhu, Kevin Flanagan, Adriano Fragomeni, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>Twelfth International Workshop on Assistive Computer Vision and Robotics ECCV, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.02335">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3>Short Bio</h3>

            Michael is a lecturer in Computer Vision at the School of Computer
            Science at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li><span class="prim-colour">Applied Deep Learning</span> 22/23, 23/24, 24/25. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                <li><span class="prim-colour">Computer Systems A</span> 22/23. 23/24, 24/25. <a href="https://uob-csa.github.io/website/">Webpage</a>.
                <li><span class="prim-colour">Individual Projects</span> 22/23. 23/24, 24/25. <a href="https://cs-uob-individual-project.github.io/">Webpage</a>.
            </ul>

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current (w/ Dima Damen)</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current (w/ Dima Damen)</li>
                <li><span class="prim-colour">Shijia Feng</a></span>: PhD, 2022&#8211;Current (w/ Walterio Mayol Cuevas)</li>
                <li><span class="prim-colour">Sam Pollard</a></span>: MEng, PhD, 2023&#8211;Current</li>
                <li><span class="prim-colour">Beth Pearson</a></span>: PhD, 2023&#8211;Current (w/ Martha Lewis)</li>
                <li><span class="prim-colour">Fahd Abdelazim</a></span>: PhD, 2024&#8211;Current </li>
                <li><span class="prim-colour">Alyssa Boisse</a></span>: MEng, 2025</li>
                <li><span class="prim-colour">Amr Khaled Mohamed El-Sawy</a></span>: MEng, 2025</li>
                <li><span class="prim-colour">Jacob Seaborn</a></span>: MEng, 2025</li>
                <li><span class="prim-colour">Aleksandra Walusiak</a></span>: MEng, 2025</li>
            </ul>

            <h4>Previous</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">Richa Banthia</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Alex Elwood</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Moise Guran</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Rahat Mittal</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Bence Szarka</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Lee Tancock</a></span>: MEng, 2023</li>
                <li><span class="prim-colour">Zac Woodford</a></span>: MEng, 2023</li>
                <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
            </ul>

            <hr>

            <h3>Misc.</h3>

            <h4>Presentations</h4>
            <ul class="no-bullets">
                <li><span class="prim-colour">BMVA Summer School</span> Egocentric Vision Lecture 2022, 2023, 2024.</li>
                <li><span class="prim-colour">VIViD Research Seminar, Durham University</span> Fine Grained Video Understanding from a Personal Perspective. 2024.</li>
                <li><span class="prim-colour">Advancements in Time Series Analysis for Computer Vision: Techniques, Applications, and Challenges</span> Unlocking the Temporal Dimension from the Egocentric Perspective 2024.</li>
                <li><span class="prim-colour">Video Understanding Symposium 2022</span> Do we still need Classification for Video Understanding? 2022.</li>
                <li><span class="prim-colour">BMVA Symposium: Robotics Meets Semantics</span> Towards an Unequivocal Representation of Actions. 2018.</li>
                <li><span class="prim-colour">EPIC@ECCV2016</span> SEMBED: Semantic Embedding of Egocentric Action Videos. 2016.</li>
            </ul>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">WINVU</span>: <a href="">CVPR2024</a></li>
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/">CVPR2023</a>, <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
                <li><span class="prim-colour">Ego4D@</span>: <a href="https://ego4d-data.org/workshops/eccv22/">ECCV2022</a></li>
            </ul>

            <h4>Area Chair</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2025, 2024, 2023</li>
                <li><span class="prim-colour">ECCV: </span>2024</li>
                <li><span class="prim-colour">NeurIPS: </span>2024</li>
            </ul>

            <h4>Associate Editor</h4>
            <ul class="no-bullets">
                <li> <span class="prim-colour">IET Computer Vision</span> 2024&#8211;Current </li>
                <li> <span class="prim-colour">ToMM Special Issue on Text-Multimedia Retrieval </span> 2024 </li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">BMVC2024</span></li>
                <li><span class="prim-colour">ECCV2022</span></li>
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>


            <h4> Reviewing Duties</h4>

            <h5> Conferences</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">NeurIPS: </span>2023</li>
                <li><span class="prim-colour">NeurIPS D&B Track: </span>2024, 2023</li>
                <li><span class="prim-colour">ICCV: </span>2023, 2021</li>
                <li><span class="prim-colour">ECCV: </span>2022</li>
                <li><span class="prim-colour">ACCV: </span>2024, 2022, 2020</li>
                <li><span class="prim-colour">BMVC: </span>2024, 2023, 2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">WACV: </span>2024, 2023, 2022, 2021</li>
            </ul>
            
            <h5> Journals</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">TPAMI</span></li>
                <li><span class="prim-colour">IJCV</span></li>
                <li><span class="prim-colour">TCSVT</span></li>
                <li><span class="prim-colour">Pattern Recognition</span></li>
                <li><span class="prim-colour">TOMM</span></li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

