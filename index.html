<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Michael Wray</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#research">Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education & Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#interests">Interests</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Michael
            <span class="text-primary">Wray</span>
          </h1>
          <div class="subheading mb-5">PhD Student . University of Bristol . 
            michael (dot) wray (at) bristol (dot) ac (dot) uk
          </div>
          <p class="mb-5">My research focuses on action recognition and understanding its links with language.</p>
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
              <a href="https://github.com/mwray">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="news">
        <div class="my-auto">
          <h2 class="mb-5">News</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Presentation at BMVA Symposium </h3>
              <div class="subheading mb-3">Presentation of Towards an Unequivocal Representation of Actions</div>
              <p>I presented a talk on Towards an Unequivocal
              Representation of Actions at BMVA Symposium: Robotics meets
              Semantics: Enabling Human-Level Understanding in Robots on 18th
              July. <a href="Towards.pptx">Slides.</a></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">EPIC Kitchens Demo at CVPR 2018</h3>
              <div class="subheading mb-3">Demo - Wednesday AM Booth 7</div>
              <p>Myself along with two other authors demoed EPIC at CVPR 2018.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Poster at BIVU2018</h3>
              <div class="subheading mb-3">Towards an Unequivocal Representation of Actions</div>
              <p>I presented a poster of Towards an Unequivocal Representation of Actions at the Brave New Ideas for Video Understanding workshop at CVPR2018.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">May 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">EPIC Kitchens</h3>
              <div class="subheading mb-3">Largest Egocentric Dataset</div>
              <p>We have just released the largest egocentric dataset for action and object recognition. More info can be found here.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">New Paper on ArXiv</h3>
              <div class="subheading mb-3">Towards an Unequivocal Representation of Actions</div>
              <p>We have released a shortform version of Towards an Unequivocal Representation of Actions on ArXiv here.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper accepted at ICCV 2017</h3>
              <div class="subheading mb-3">Trespassing the boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</div>
              <p>Davide Moltisanti presented the poster for the paper in ICCV2017.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2017</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper accepted at EPIC Workshop</h3>
              <div class="subheading mb-3">SEMBED: Semantic Embedding of Egocentric Action Videos</div>
              <p>Our paper SEMBED was presented at the first EPIC workshop during ECCV2016.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2016</span>
            </div>
          </div>

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-5">Research</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Towards an Unequivocal Representation of Actions</h3>
              <div class="subheading mb-3"><a href = "https://arxiv.org/abs/1805.04026">Arxiv</a></div>
              <p>This work introduces verb-only representations for actions and
              interactions; the problem of describing similar motions (e.g.
              'open door', 'open cupboard'), and distinguish differing ones
              (e.g. 'open door' vs 'open bottle') using verb-only labels.
              Current approaches for action recognition neglect legitimate
              semantic ambiguities and class overlaps between verbs, relying on
              the objects to disambiguate interactions.</p>

              <div class="modal fade" id="modalTowards" role="dialog">
                <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                  <div class="modal-content">
                    <div class="modal-body">
                      <h3 class="mb-0">Towards an Unequivocal Representation of Actions</h3>
                      <p>This work introduces verb-only representations for
                      actions and interactions; the problem of describing
                      similar motions (e.g.  'open door', 'open cupboard'), and
                      distinguish differing ones (e.g. 'open door' vs 'open
                      bottle') using verb-only labels.  Current approaches for
                      action recognition neglect legitimate semantic
                      ambiguities and class overlaps between verbs, relying on
                      the objects to disambiguate interactions.</p>

                      <p> We deviate from single-verb labels and introduce a
                      mapping between observations and multiple verb labels -
                      in order to create an Unequivocal Representation of
                      Actions. The new representation benefits from increased
                      vocabulary and a soft assignment to an enriched space of
                      verb labels.  </p>
                  
                      <p> We learn these representations as multi-output
                      regression, using a two-stream fusion CNN. The proposed
                      approach outperforms conventional single-verb labels
                      (also known as majority voting) on three egocentric
                      datasets for both recognition and retrieval.  </p>

                      <h3 class="mb-0">Publication:</h3>
                      Towards an Unequivocal Representation of Actions, Michael Wray, Davide Moltisanti and Dima Damen (2018).
                      <b>Link:</b> <a href = "https://arxiv.org/abs/1805.04026">Arxiv</a>.
                      

                      <h3 class="mb-0">Video:</h3>
                      <p><iframe width="560" height="315" src="https://www.youtube.com/embed/Taz9dfY_vd0" frameborder="0" allowfullscreen></iframe></p>

                      <h3 class="mb-0">Results</h3>
                      Example cross dataset retrievals using a soft assigned multi label approach.
                      <p><img src="Towards/results.png" alt="Results" width=900/></p>

                    </div>
                    <div class="modal-footer">
                      <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                    </div>
                  </div>
                  
                </div>
              </div>
                </div>
                <div class="resume-date text-md-right">
                  <span class="text-primary">May 2018</span>
                  </br>
                  <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalTowards">More Info</button>
                </div>
              </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
              <div class="subheading mb-3"><a href = "https://arxiv.org/abs/1804.02748">Arxiv</a></div>
              <p>First-person vision is gaining interest as it offers a unique
              viewpoint on people's interaction with objects, their attention,
              and even intention. However, progress in this challenging domain
              has been relatively slow due to the lack of sufficiently large
              datasets. In this paper, we introduce EPIC-KITCHENS, a
              large-scale egocentric video benchmark recorded by 32
              participants in their native kitchen environments. Our videos
              depict nonscripted daily activities: we simply asked each
              participant to start recording every time they entered their
              kitchen</p>
            </div>

            <div class="modal fade" id="modalScaling" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
                    <p>The largest dataset in first-person (egocentric)
                    vision; multi-faceted non-scripted recordings in native
                    environments - i.e. the wearers' homes, capturing all
                    daily activities in the kitchen over multiple days.
                    Annotations are collected using a novel `live' audio
                    commentary approach.</p>

                    <h3 class="mb-0">Publication:</h3>
                    Scaling Egocentric Vision: The EPIC-KITCHENS Dataset.
                    Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
                    Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide
                    Moltisanti, Jonathan Munro, Toby Perrett, Will Price,
                    Michael Wray (2018)
                    </br>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1804.02748">Arxiv</a>.
                    

                    <h3 class="mb-0">Video:</h3>
                    <p><iframe width="560" height="315" src="https://www.youtube.com/embed/Dj6Y3H0ubDw" frameborder="0" allowfullscreen></iframe></p>

                    <h3 class="mb-0">Website</h3>
                    <p> The official website of EPIC-Kitchens can be found 
                    <a href = "https://epic-kitchens.github.io/2018">here.</a>
                    This includes links to download the dataset and
                    annotations.</p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalScaling">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</h3>
              <div class="subheading mb-3"><a href = "http://openaccess.thecvf.com/content_ICCV_2017/papers/Moltisanti_Trespassing_the_Boundaries_ICCV_2017_paper.pdf">ICCV 2017</a></div>
              <p>Manual annotations of temporal bounds for object interactions
              (i.e. start and end times) are typical training input to
              recognition, localization and detection algorithms. For three
              publicly available egocentric datasets, we uncover
              inconsistencies in ground truth temporal bounds within and across
              annotators and datasets. We systematically assess the robustness
              of state-of-the-art approaches to changes in labeled temporal
              bounds, for object interaction recognition.</p>
            </div>

            <div class="modal fade" id="modalTrespassing" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</h3>
                    <p>Manual annotations of temporal bounds for object
                    interactions (i.e. start and end times) are typical
                    training input to recognition, localization and detection
                    algorithms. For three publicly available egocentric
                    datasets, we uncover inconsistencies in ground truth
                    temporal bounds within and across annotators and datasets.
                    We systematically assess the robustness of state-of-the-art
                    approaches to changes in labeled temporal bounds, for
                    object interaction recognition.</p>

                    <h3 class="mb-0">Publication:</h3>
                    Trespassing the Boundaries: Labeling Temporal Bounds for
                    Object Interactions in Egocentric Video. Davide Moltisanti,
                    Michael Wray and Dima Damen(2017).
                    </br>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1703.09026">Arxiv</a>.
                    
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2017</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalTrespassing">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">sembed: semantic embedding of egocentric action videos</h3>
              <div class="subheading mb-3">ECCVW 2016 - <a href= "./pdfs/ECCVW2016SEMBED.pdf">pdf</a></div>
              <p>we present sembed, an approach for embedding an egocentric
              object interaction video in a semantic-visual graph to estimate
              the probability distribution over its potential semantic
              labels.</p>
            </div>

            <div class="modal fade" id="modalSembed" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">SEMBED: Semantic Embedding of Egocentric Action Videos</h3>
                    <p> Egocentric Action Recognition has largely been
                    performed on datasets where annotators had a choice from a
                    finite set of semantically distinct verbs.  </p>
                    
                    <p> When allowed free choice over both the verb chosen to
                    describe an action and the temporal boundaries the
                    resulting dataset contains a wide variety of different
                    labels. Whilst these labels are diverse we treat them all
                    as correct labels. Standard one vs. all classifying
                    techniques, such as SVM, are unable to deal with the
                    ambiguities introduced by the free annotations and so a
                    graphical approach is introduced.  </p>
                    
                    <p> We present SEMBED, a method which is capable of dealing
                    with the ambiguity within the dataset by embedding videos
                    in a Semantic Visual Graph. Multiple state-of-the-art
                    features have been tested (in the form of Improved Dense
                    Trajectories and Overfeat CNN) along with Bag of (Visual)
                    Words and Fisher Vectors as encoding methods. We find that
                    SEMBED using the notion of embedding videos in a graph that
                    are linked visually and/or semantically is able to beat SVM
                    by more than 5%.  </p>

                    <h3 class="mb-0">Publication:</h3>
                    SEMBED: Semantic Embedding of Egocentric Action Videos.
                    Michael Wray(*), Davide Moltisanti(*), Walterio
                    Mayol-Cuevas and Dima Damen(2016). <i> In Egocentric 
                    Perception, Interaction and Computing Workshop ECCV 2016, 
                    Amsterdam, The Netherlands.</i>
                    <p><i>* Denotes equal contribution</i></p>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1607.08414">Arxiv</a>.

                    <h3 class="mb-0">Video:</h3>
                    <p><iframe width="560" height="315" src="https://www.youtube.com/embed/6bDDTIJUuic" frameborder="0" allowfullscreen></iframe></p>

                    <h3>Dataset:</h3>
                    <a href="http://www.cs.bris.ac.uk/~Damen/BEOID/">Bristol Egocentric Object Interactions Dataset</a>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>

            <div class="resume-date text-md-right">
              <span class="text-primary">october 2016</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalSembed">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">The Cage: Towards a 6-DoF Remote Control with Force Feedback for UAV Interaction.</h3>
              <div class="subheading mb-3">Extended Abstract CHI 2015</div>
              <p>Unmanned Aerial Vehicles (UAVs) require complex control and
              significant experience for piloting. While these devices continue
              to improve, there is, as yet, no device that affords six degrees
              of freedom (6-DoF) control and directional haptic feedback. We
              present The Cage, a 6-DoF controller for piloting an unmanned
              aerial vehicle (UAV).</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2015</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
        <div class="my-auto">
          <h2 class="mb-5">Education and Experience</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">University of Bristol</h3>
              <div class="subheading mb-3">PhD in Computer Vision</div>
              <p>Ongoing</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">September 2015 - Current</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Cisco</h3>
              <div class="subheading mb-3">Internship</div>
              <div>Router Testing/Development</div>
              <p>3 Month Internship</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2014 - August 2014</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">University of Bristol</h3>
              <div class="subheading mb-3">Master of Engineering</div>
              <div>Computer Science</div>
              <p>First Class</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">September 2011 - May 2015</span>
            </div>
          </div>


        </div>
      </section>


      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="interests">
        <div class="my-auto">
          <h2 class="mb-5">Interests</h2>
          <p>Whilst not working on completing my PhD I enjoy reading - primarily Science Fiction and Fantasy. Below are few books/series I would recommend:</p>
          <ul>
              <li>Wheel of Time - Robert Jordan/Brandon Sanderson.</li>
              <li>Lightbringer - Brent Weeks.</li>
              <li>Rendezvous with Rama - Arthur C. Clarke.</li>
          </ul>
          <p class="mb-0"></p>
        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/website.min.js"></script>

  </body>

</html>
