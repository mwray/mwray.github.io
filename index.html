<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Lecturer of Computer Vision</h2>
            </header>

                <p>I am a lecturer of Computer Vision at the Department of Computer
                Science at the University of Bristol. My research interests are in
                multi-modal video understanding, particularly for egocentric
                videos â€” focusing on how both vision and language can be tied
                together towards tasks such as cross-modal retrieval, grounding
                and captioning.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">July 2022</span> - <b>BMVA Summer School Lecture on Egocentric Vision</b> - I presented a lecture on Egocentric Vision at the <a href="https://cvss2022-uea.uk/">BMVA Summer School</a> at UEA. A copy of the slides can be found <a href="https://uob-my.sharepoint.com/:p:/g/personal/mw1760_bristol_ac_uk/EYjImN8DnRBMjUSIz4x16QMBVhzWcyprUCX-j8oq8nSLLQ?e=9kDTvS">here.</a>
                <li> <span class="prim-colour">June 2022</span> - <b>Research Lecture @ University of Catania</b> - I presented a lecture on my research at the University of Catania.
                <li> <span class="prim-colour">June 2022</span> - <b>Russell Group Teacher Conference Keynote</b> - I gave the opening keynote at the Russell Group teacher conference about my journey from PhD student to lecturer.
                <li> <span class="prim-colour">March 2022</span> - <b>Paper at CVPR2022 Accepted</b> - Our paper "Ego4D: Around the World in 3,000 Hours of Egocentric Video" was accepted at CVPR2022.
                <li> <span class="prim-colour">March 2022</span> - <b>Lecturer Position</b> - I joined the Department of Computer Science as a Lecturer in Computer Vision.
                <li> <span class="prim-colour">December 2021</span> - <b>Workshop at CVPR2022 Accepted</b> - Joint Ego4D & EPIC Workshop@CVPR 2022 accepted, <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">website</a>.
                <li> <span class="prim-colour">October 2021</span> - <b>Paper Accepted at IJCV</b> - Rescaling Egocentric Vision has been accepted to IJCV, find more information <a href="https://epic-kitchens.github.io/2021">here</a>.</li>
                <li> <span class="prim-colour">August 2021</span> - <b>Outstanding Reviewer at ICCV 2021</b>
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of Research projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/egovlp.jpeg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Egocentric Video-Language Pretraining</span>
                            <br>
                            Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia
                            Soldan, <span>Michael Wray</span>, Rui Yan, Eric
                            Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao,
                            Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen,
                            Bernard Ghanem, Wei Liu, Mike Zheng Shou
                            <br>
                            <em>ArXiv, 2022</em>
                            <br>
                            <strong>
                                <a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2206.01670">[arXiv]</a>
                                <a href="https://github.com/showlab/EgoVLP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ego4d.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego4D: Around the World in 3,000 Hours of Egocentric Video</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, <span>Michael Wray</span>, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR) 2022</em>
                            <br>
                            <strong>
                                <a href="https://ego4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2110.07058">[arXiv]</a>
                                <a href="https://ego4d-data.org/docs/">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/DACMR.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval</span>
                            <br>
                            Jonathan Munro, <span>Michael Wray</span>, Diane
                            Larlus, Gabriela Csurka, Dima Damen
                            <br>
                            <em>ArXiv, 2021</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2110.12812">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/epic_kitchens.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Rescaling Egocentric Vision</span>
                            <br>
                            Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and <span>Michael Wray</span>
                            <br>
                            <em>Springer International Journal of Computer Vision (IJCV), 2021</em>
                            <br>
                            <strong>
                                <a href="https://epic-kitchens.github.io/">[Webpage]</a>
                                <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ssvr_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">On Semantic Similarity in Video Retrieval</span>
                            <br>
                            <span>Michael Wray</span>, Hazel Doughty, and Dima Damen
                            <br>
                            <em>IEEE Conference on Computer Vision and Pattern Recognition, 2021.</em>
                            <br>
                            <strong>
                                <a href="https://mwray.github.io/SSVR/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2103.10095">[arXiv]</a>
                                <a href="https://github.com/mwray/Semantic-Video-Retrieval">[Code]</a>
                                <a href="https://mwray.github.io/SSVR/SSVR.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/sls.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Supervision Level Scales</span>
                            <br>
                            Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>arXiv, 2020</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2008.09890">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3>Short Bio</h3>

            Michael is a lecturer in Computer Vision at the Computer Science
            Department at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current</li>
            </ul>

            <h4>Previous</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
            </ul>

            <hr>

            <h3>Misc.</h3>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>
            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

