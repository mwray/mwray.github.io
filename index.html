<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.png" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Assistant Professor of Computer Vision</h2>
            </header>

                <p>I am a lecturer/Assistant Professor of Computer Vision at
                the School of Computer Science at the University of
                Bristol. My research interests are in multi-modal video
                understanding, particularly for egocentric videos â€” focusing on
                how both vision and language can be tied together towards tasks
                such as cross-modal retrieval, grounding and captioning.
                I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
                <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>.

                <a href="#shortbio">Short Bio</a>.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://twitter.com/mwray0">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">December 2025</span> - <b>WACV Paper on ArXiv</b> Our paper: "From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities" that will be presented at WACV is now on <a href="https://arxiv.org/abs/2512.09847">ArXiv</a>.
                <li> <span class="prim-colour">November 2025</span> - <b>Successful PhD Defence</b> Congratulations to Shijia for passing their PhD defence!
                <li> <span class="prim-colour">November 2025</span> - <b>Successful PhD Defence</b> Congratulations to Kevin for passing their PhD defence!
                <li> <span class="prim-colour">November 2025</span> - <b>Two Papers Accepted at WACV</b> Congratulations to Shijia and Wiktor. Papers will be on ArXiv soon!
                <li> <span class="prim-colour">October 2025</span> - <b>New Paper on ArXiv</b> Our paper: "A Video Is Not Worth a Thosand Words" is now on ArXiv, <a href="https://sjpollard.github.io/a-video-is-not-worth-a-thousand-words/">Website</a>, <a href="https://arxiv.org/abs/2510.23253">ArXiv</a>.
                <li> <span class="prim-colour">October 2025</span> - <b>New Paper on ArXiv</b> Our paper: "EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels" is now on ArXiv, <a href="https://github.com/FELIXFENG2019/EvoStruggle">Dataset</a>, <a href="https://arxiv.org/abs/2510.01362">ArXiv</a>.
                <li> <span class="prim-colour">August 2025</span> - <b>Paper Accepted at *SEM 2025 </b> Our paper: "Evaluating Compositional Generalisation in VLMs and Diffusion Models" has been accepted at *SEM, <a href="https://arxiv.org/abs/2508.20783">ArXiv</a>.
                <li> <span class="prim-colour">August 2025</span> - <b>Paper Accepted at BMVC 2025 </b> Our paper: "Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval" has been accepted at BMVC, <a href="https://arxiv.org/abs/2504.01591">ArXiv</a>.
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of recent Research Projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/online_struggle_intro.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities</span>
                            <br>
                            Shijia Feng, <span>Michael Wray</span>, Walterio Mayol-Cuevas
                            <br>
                            <em>WACV, 2026</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2512.09847">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/vis_shap_intro.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">A Video Is Not Worth a Thousand Words</span>
                            <br>
                            Sam Pollard, <span>Michael Wray</span>
                            <br>
                            <em>ArXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://sjpollard.github.io/a-video-is-not-worth-a-thousand-words/">[Website]</a>
                                <a href="https://arxiv.org/abs/2510.23253">[arXiv]</a>
                                <a href="https://github.com/sjpollard/a-video-is-not-worth-a-thousand-words">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/diff_comp_intro.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Evaluating Compositional Generalisation in VLMs and Diffusion Models</span>
                            <br>
                            Beth Pearson, Bilal Boulbarss, <span>Michael Wray</span>, Martha Lewis
                            <br>
                            <em>*SEM, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2508.20783">[arXiv]</a>
                                <a href="https://github.com/otmive/diffusion_classifier_clip">[Code]</a>
                                <a href="https://drive.google.com/file/d/14U4azHV6FHI8yeALWfgKvnH40OpDvbz1/view?usp=sharing">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/tags_CMVR_intro.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval</span>
                            <br>
                            Adriano Fragomeni, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>BMVC, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2504.01591">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/EvoStruggle_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels</span>
                            <br>
                            Shijia Feng, <span>Michael Wray</span>, Walterio Mayol-Cuevas
                            <br>
                            <em>ArXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2510.01362">[arXiv]</a>
                                <a href="https://github.com/FELIXFENG2019/EvoStruggle">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/struggle-illustration.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos</span>
                            <br>
                            Shijia Feng, <span>Michael Wray</span>, Brian Sullivan, Youngkyoon Jang, Casimir Ludwig, Iain Gilchrist, Walterio Mayol-Cuevas 
                            <br>
                            <em>IJCV, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.11057">[arXiv]</a>
                                <a href="https://github.com/FELIXFENG2019/Struggle-Determination">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3 id="shortbio">Short Bio</h3>

            Michael is a lecturer in Computer Vision at the School of Computer
            Science at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li><span class="prim-colour">Applied Deep Learning</span> 22/23, 23/24, 24/25, 25/26. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                <li><span class="prim-colour">Computer Systems A</span> 22/23. 23/24, 24/25, 25/26. <a href="https://uob-csa.github.io/website/">Webpage</a>.
                <li><span class="prim-colour">Individual Projects</span> 22/23. 23/24, 24/25. <a href="https://cs-uob-individual-project.github.io/">Webpage</a>.
            </ul>

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current (w/ Dima Damen)</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current (w/ Dima Damen)</li>
                <li><span class="prim-colour">Shijia Feng</a></span>: PhD, 2022&#8211;Current (w/ Walterio Mayol Cuevas)</li>
                <li><span class="prim-colour"><a href ="https://sjpollard.github.io/">Sam Pollard</a></span>: MEng, PhD, 2023&#8211;Current</li>
                <li><span class="prim-colour">Beth Pearson</a></span>: PhD, 2023&#8211;Current (w/ Martha Lewis)</li>
                <li><span class="prim-colour"><a href = "https://fahdabdelazim.github.io/">Fahd Abdelazim</a></span>: PhD, 2024&#8211;Current </li>
            </ul>

            <h4>Previous</h4>

            <div class="scroll-box">
                <ul class="no-bullets">
                    <li><span class="prim-colour">Alyssa Boisse</a></span>: MEng, 2025</li>
                    <li><span class="prim-colour">Isabel Mendes Rodrigues</a></span>: BSc, 2025</li>
                    <li><span class="prim-colour">Amr Khaled Mohamed El-Sawy</a></span>: BSc, 2025</li>
                    <li><span class="prim-colour">Jacob Seaborn</a></span>: MEng, 2025</li>
                    <li><span class="prim-colour">Aleksandra Walusiak</a></span>: MEng, 2025</li>
                    <li><span class="prim-colour">Richa Banthia</a></span>: MEng, 2024</li>
                    <li><span class="prim-colour">Alex Elwood</a></span>: MEng, 2024</li>
                    <li><span class="prim-colour">Moise Guran</a></span>: MEng, 2024</li>
                    <li><span class="prim-colour">Rahat Mittal</a></span>: BSc, 2024</li>
                    <li><span class="prim-colour">Bence Szarka</a></span>: MEng, 2024</li>
                    <li><span class="prim-colour">Lee Tancock</a></span>: MEng, 2023</li>
                    <li><span class="prim-colour">Zac Woodford</a></span>: MEng, 2023</li>
                    <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                    <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
                </ul>
            </div>

            <hr>

            <h3>Misc.</h3>

            <h4>Presentations</h4>
            <ul class="no-bullets">
                <li><span class="prim-colour">BMVA Summer School</span> Egocentric Vision Lecture <a target="_blank" href="https://uob-my.sharepoint.com/:p:/g/personal/mw1760_bristol_ac_uk/EXBBSh1gYWtGihWkguefLToBbaY1PRxEWY3DDWg67Mr7EA?e=9lsgJv">2025</a>, 2024, 2023, 2022.</li>
                <li><span class="prim-colour">VIViD Research Seminar, Durham University</span> Fine Grained Video Understanding from a Personal Perspective. 2024.</li>
                <li><span class="prim-colour">Advancements in Time Series Analysis for Computer Vision: Techniques, Applications, and Challenges</span> Unlocking the Temporal Dimension from the Egocentric Perspective 2024.</li>
                <li><span class="prim-colour">Video Understanding Symposium 2022</span> Do we still need Classification for Video Understanding? 2022.</li>
                <li><span class="prim-colour">BMVA Symposium: Robotics Meets Semantics</span> Towards an Unequivocal Representation of Actions. 2018.</li>
                <li><span class="prim-colour">EPIC@ECCV2016</span> SEMBED: Semantic Embedding of Egocentric Action Videos. 2016.</li>
            </ul>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">WINVU@</span>: <a href="https://winvu.github.io/cvpr-24/">CVPR2024</a></li>
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/">CVPR2023</a>, <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
                <li><span class="prim-colour">Ego4D@</span>: <a href="https://ego4d-data.org/workshops/eccv22/">ECCV2022</a></li>
            </ul>

            <h4>Area Chair</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2025, 2024, 2023</li>
                <li><span class="prim-colour">ECCV: </span>2024</li>
                <li><span class="prim-colour">NeurIPS: </span>2025, 2024</li>
            </ul>

            <h4>Associate Editor</h4>
            <ul class="no-bullets">
                <li> <span class="prim-colour">IET Computer Vision</span> 2024&#8211;Current </li>
                <li> <span class="prim-colour">ToMM Special Issue on Text-Multimedia Retrieval </span> 2024 </li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">BMVC2024</span></li>
                <li><span class="prim-colour">ECCV2022</span></li>
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>


            <h4> Reviewing Duties</h4>

            <h5> Conferences</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">AAAI: </span>2026</li>
                <li><span class="prim-colour">ACM-MM: </span>2025</li>
                <li><span class="prim-colour">ACCV: </span>2024, 2022, 2020</li>
                <li><span class="prim-colour">BMVC: </span>2025, 2024, 2023, 2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">CVPR: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">ECCV: </span>2022</li>
                <li><span class="prim-colour">ICCV: </span>2025, 2023, 2021</li>
                <li><span class="prim-colour">NeurIPS: </span>2023</li>
                <li><span class="prim-colour">NeurIPS D&B Track: </span>2024, 2023</li>
                <li><span class="prim-colour">WACV: </span>2026, 2024, 2023, 2022, 2021</li>
            </ul>
            
            <h5> Journals</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">TPAMI</span></li>
                <li><span class="prim-colour">IJCV</span></li>
                <li><span class="prim-colour">TCSVT</span></li>
                <li><span class="prim-colour">Pattern Recognition</span></li>
                <li><span class="prim-colour">TOMM</span></li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

