<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Assistant Professor of Computer Vision</h2>
            </header>

                <p>I am a lecturer/Assistant Professor of Computer Vision at
                the School of Computer Science at the University of
                Bristol. My research interests are in multi-modal video
                understanding, particularly for egocentric videos — focusing on
                how both vision and language can be tied together towards tasks
                such as cross-modal retrieval, grounding and captioning.
                I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
                <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://twitter.com/mwray0">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">February 2024</span> - <b>Two Papers Accepted at CVPR2024</b> <a href="https://soczech.github.io/genhowto/">GenHowTo</a> and <a href="https://ego-exo4d-data.org/">EgoExo4D</a> have been accepted at CVPR.
                <li> <span class="prim-colour">January 2024</span> - <b>CVPR Workshop on the future of Video</b> Co-Organising the "What is Next in Video Understanding workshop?" at CVPR 2024.
                <li> <span class="prim-colour">January 2024</span> - <b>New Papers on ArXiv</b> Two new papers are on arXiv "GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos" (<a href="https://soczech.github.io/genhowto/">here</a>) and "Video Editing for Video Retrieval" (<a href="https://arxiv.org/abs/2402.02335">here</a>).
                <li> <span class="prim-colour">November 2023</span> - <b>Ego-Exo-4D Released</b> - Our dataset Ego-Exo-4D has been released. Find more information <a href="https://ego-exo4d-data.org/">here</a>.
                <li> <span class="prim-colour">October 2023</span> - <b>Area Chair at ECCV 2024</b> - Honoured to serve as an area chair for ECCV 2024.</li>
                <li> <span class="prim-colour">September 2023</span> - <b>Paper Accepted at BMVC 2023</b> - Our paper titled "Learning Temporal Sentence Grounding From Narrated EgoVideos" was accepted at BMVC 2023. See more <a href="https://keflanagan.github.io/CliMer-TSG/">here</a>.
                <li> <span class="prim-colour">September 2023</span> - <b>Call for Presentations for V+L symposium</b> - We are currently looking for presentations at the BMVA 1-day symposium on Vision and Language. Find out more <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">here</a> and <a href="https://forms.gle/dJ5uw5i5F7jBQi977">apply here</a>.</li>
                <li> <span class="prim-colour">September 2023</span> - <b>Organising a 1-day symposium on Vision and Language</b> - I am one of the organisers for a 1-day event in Vision and Language taking place in January 2024. More information <a href="https://www.bmva.org/meetings/24-01-17-Vision%20and%20Language.html">here</a>.</li>
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of Research projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/video_edit_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Video Editing for Video Retrieval</span>
                            <br>
                            Bin Zhu, Kevin Flanagan, Adriano Fragomeni, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>arXiv, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.02335">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/genhowto_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos</span>
                            <br>
                            Tomáš Souček, Dima Damen, <span>Michael Wray</span>, Ivan Laptev, Josef Sivic
                            <br>
                            <em>arXiv, 2024</em>
                            <br>
                            <strong>
                                <a href="https://soczech.github.io/genhowto/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2312.07322">[arXiv]</a>
                                <a href="https://github.com/soCzech/genhowto">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ego-exo4d_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Lorenzo
                            Torresani, Kris Kitani, Jitendra Malik,
                            Triantafyllos Afouras, Kumar Ashutosh, Vijay
                            Baiyya, Siddhant Bansal, Bikram Boote, Eugene
                            Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen
                            Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria
                            Escobar, Cristhian Forigua, Abrham Gebreselasie,
                            Sanjay Haresh, Jing Huang, Md Mohaiminul Islam,
                            Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin
                            J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao,
                            Miguel Martin, Effrosyni Mavroudi, Tushar
                            Nagarajan, Francesco Ragusa, Santhosh Kumar
                            Ramakrishnan, Luigi Seminara, Arjun Somayazulu,
                            Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu
                            Zhang, Angela Castillo, Changan Chen, Xinzhu Fu,
                            Ryosuke Furuta, Cristina Gonzalez, Prince Gupta,
                            Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo,
                            Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu,
                            Mi Luo, Zhengyi Luo, Brighid Meredith, Austin
                            Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny
                            Peng, Shraman Pramanick, Merey Ramazanova, Fiona
                            Ryan, Wei Shan, Kiran Somasundaram, Chenan Song,
                            Audrey Southerland, Masatoshi Tateno, Huiyu Wang,
                            Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang,
                            Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei
                            Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas
                            Bertasius, David Crandall, Dima Damen, Jakob Engel,
                            Giovanni Maria Farinella, Antonino Furnari, Bernard
                            Ghanem, Judy Hoffman, C. V. Jawahar, Richard
                            Newcombe, Hyun Soo Park, James M. Rehg, Yoichi
                            Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou,
                            <span>Michael Wray</span>
                            <br>
                            <em>arXiv, 2023</em>
                            <br>
                            <strong>
                                <a href="https://ego-exo4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2311.18259">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/climer_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Learning Temporal Sentence Grounding From Narrated EgoVideos</span>
                            <br>
                            Kevin Flanagan, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>BMVC, 2023</em>
                            <br>
                            <strong>
                                <a href="https://keflanagan.github.io/CliMer-TSG/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2310.17395">[arXiv]</a>
                                <a href="https://github.com/keflanagan/CliMer">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/contra_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval</span>
                            <br>
                            Adriano Fragomeni, <span>Michael Wray</span>, Dima
                            Damen,
                            <br>
                            <em>ACCV (Oral), 2022</em>
                            <br>
                            <strong>
                                <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2210.04341">[arXiv]</a>
                                <a href="https://github.com/adrianofragomeni/ConTra">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/egovlp.jpeg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Egocentric Video-Language Pretraining</span>
                            <br>
                            Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia
                            Soldan, <span>Michael Wray</span>, Rui Yan, Eric
                            Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao,
                            Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen,
                            Bernard Ghanem, Wei Liu, Mike Zheng Shou
                            <br>
                            <em>NeurIPS, 2022</em>
                            <br>
                            <strong>
                                <a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2206.01670">[arXiv]</a>
                                <a href="https://github.com/showlab/EgoVLP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3>Short Bio</h3>

            Michael is a lecturer in Computer Vision at the School of Computer
            Science at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li><span class="prim-colour">Applied Deep Learning</span> 22/23, 23/24. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                <li><span class="prim-colour">Computer Systems A</span> 22/23. 23/24. <a href="https://uob-csa.github.io/website/">Webpage</a>.
                <li><span class="prim-colour">Individual Projects</span> 22/23. 23/24. <a href="https://cs-uob-individual-project.github.io/">Webpage</a>.
            </ul>

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current (w/ Dima Damen)</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current (w/ Dima Damen)</li>
                <li><span class="prim-colour">Shijia Feng</a></span>: PhD, 2022&#8211;Current (w/ Walterio Mayol Cuevas)</li>
                <li><span class="prim-colour">Sam Pollard</a></span>: MEng, PhD, 2023&#8211;Current</li>
                <li><span class="prim-colour">Beth Pearson</a></span>: PhD, 2022&#8211;Current (w/ Martha Lewis)</li>
                <li><span class="prim-colour">Richa Banthia</a></span>: MEng, 2024&#8211;Current</li>
                <li><span class="prim-colour">Alex Elwood</a></span>: MEng, 2024&#8211;Current</li>
                <li><span class="prim-colour">Moise Guran</a></span>: MEng, 2024&#8211;Current</li>
                <li><span class="prim-colour">Rahat Mittal</a></span>: MEng, 2024&#8211;Current</li>
                <li><span class="prim-colour">Bence Szarka</a></span>: MEng, 2024&#8211;Current</li>
            </ul>

            <h4>Previous</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
                <li><span class="prim-colour">Lee Tancock</a></span>: MEng, 2023</li>
                <li><span class="prim-colour">Zac Woodford</a></span>: MEng, 2023</li>
            </ul>

            <hr>

            <h3>Misc.</h3>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">WINVU</span>: <a href="">CVPR2024</a></li>
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/">CVPR2023</a>, <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
                <li><span class="prim-colour">Ego4D@</span>: <a href="https://ego4d-data.org/workshops/eccv22/">ECCV2022</a></li>
            </ul>

            <h4>Area Chair</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2024, 2023</li>
                <li><span class="prim-colour">ECCV: </span>2024</li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">ECCV2022</span></li>
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>


            <h4> Reviewing Duties</h4>

            <h5> Conferences</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">NeurIPS: </span>2023</li>
                <li><span class="prim-colour">ICCV: </span>2023, 2021</li>
                <li><span class="prim-colour">ECCV: </span>2022</li>
                <li><span class="prim-colour">ACCV: </span>2022, 2020</li>
                <li><span class="prim-colour">BMVC: </span>2023, 2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">WACV: </span>2023, 2022, 2021</li>
            </ul>
            
            <h5> Journals</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">TPAMI</span></li>
                <li><span class="prim-colour">IJCV</span></li>
                <li><span class="prim-colour">TCSVT</span></li>
                <li><span class="prim-colour">Pattern Recognition</span></li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

