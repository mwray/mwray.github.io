<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <header>
                <hr>
                <h1>Research</h1>
            </header>
            <em>To return home, click <a href="index.html">here</a>.</em>
            <!--research_section-->
            <table class="researchtable">
                <tbody>
                    <tr>
                        <td class=img>
                            <img src="img/struggle-illustration.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Are you Struggling? Dataset and Baselines for Struggle Determination in Assembly Videos</span>
                            <br>
                            Shijia Feng, <span>Michael Wray</span>, Brian Sullivan, Youngkyoon Jang, Casimir Ludwig, Iain Gilchrist, Walterio Mayol-Cuevas 
                            <br>
                            <em>IJCV, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.11057">[arXiv]</a>
                                <a href="https://github.com/FELIXFENG2019/Struggle-Determination">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/token_merging.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Video, How Do Your Tokens Merge?</span>
                            <br>
                            Sam Pollard, <span>Michael Wray</span>
                            <br>
                            <em>CVPRW, 2025</em>
                            <br>
                            <strong>
                                <a href="https://sjpollard.github.io/video-how-do-your-tokens-merge/">[Website]</a>
                                <a href="https://arxiv.org/abs/2506.03885">[arXiv]</a>
                                <a href="https://github.com/sjpollard/video-how-do-your-tokens-merge">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/TVR_survey.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Leveraging Auxiliary Information in Text-to-Video Retrieval: A Review</span>
                            <br>
                            Adriano Fragomeni, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>arXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2505.23952">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/tags_CMVR_intro.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval</span>
                            <br>
                            Adriano Fragomeni, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>arXiv, 2025</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2504.01591">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/hd-epic_intro_fig.jpg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HD-EPIC: A Highly Detailed Egocentric Dataset</span>
                            <br>
                            Toby Perrett, Ahmad Darkhalil, Saptarshi Sinha, Omar Emara, Sam Pollard, Kranti Parida, Kaiting Liu, Prajwal Gatti, Siddhant Bansal, Kevin Flanagan, Jacob Chalk, Zhifan Zhu, Rhodri Guerrier, Fahd Abdelazim, Bin Zhu, Davide Moltisanti, <span>Michael Wray</span>, Hazel Doughty, Dima Damen                            <br>
                            <em>CVPR, 2025</em>
                            <br>
                            <strong>
                                <a href="https://hd-epic.github.io/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2502.04144">[arXiv]</a>
                                <a href="https://github.com/hd-epic/hd-epic-annotations">[Annotations]</a>
                                <a href="https://uob-my.sharepoint.com/:f:/g/personal/xy23932_bristol_ac_uk/Er39VjjlqKJFkNXp_tYdDaYBKEPnxNSz9GQ-VvXFW-yWWQ?e=555IHx">[Videos]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/untruth_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval</span>
                            <br>
                            Kevin Flanagan, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>WACV, 2025</em>
                            <br>
                            <strong>
                                <a href="https://keflanagan.github.io/Moment-of-Untruth/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2502.08544">[arXiv]</a>
                                <a href="https://github.com/keflanagan/MomentofUntruth">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/showhowto_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions</span>
                            <br>
                            Tomáš Souček, Prajwal Gatti, <span>Michael Wray</span>, Ivan Laptev, Dima Damen, Josef Sivic
                            <br>
                            <em>CVPR, 2025</em>
                            <br>
                            <strong>
                                <a href="https://soczech.github.io/showhowto/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2412.01987">[arXiv]</a>
                                <a href="https://github.com/soCzech/showhowto">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/SHARP_qualitative.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</span>
                            <br>
                            Wiktor Mucha, <span>Michael Wray</span>, Martin Kampel
                            <br>
                            <em>ICPR, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2408.10037">[arXiv]</a>
                                <a href="https://github.com/wiktormucha/SHARP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/hoi-ref_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision</span>
                            <br>
                            Siddhant Bansal, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>arXiv, 2024</em>
                            <br>
                            <strong>
                                <a href="https://sid2697.github.io/hoi-ref/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2404.09933">[arXiv]</a>
                                <a href="https://github.com/Sid2697/HOI-Ref">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/video_edit_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Video Editing for Video Retrieval</span>
                            <br>
                            Bin Zhu, Kevin Flanagan, Adriano Fragomeni, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>Twelfth International Workshop on Assistive Computer Vision and Robotics ECCV, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.02335">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/genhowto_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos</span>
                            <br>
                            Tomáš Souček, Dima Damen, <span>Michael Wray</span>, Ivan Laptev, Josef Sivic
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2024</em>
                            <br>
                            <strong>
                                <a href="https://soczech.github.io/genhowto/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2312.07322">[arXiv]</a>
                                <a href="https://github.com/soCzech/genhowto">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/ego-exo4d_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Lorenzo
                            Torresani, Kris Kitani, Jitendra Malik,
                            Triantafyllos Afouras, Kumar Ashutosh, Vijay
                            Baiyya, Siddhant Bansal, Bikram Boote, Eugene
                            Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen
                            Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria
                            Escobar, Cristhian Forigua, Abrham Gebreselasie,
                            Sanjay Haresh, Jing Huang, Md Mohaiminul Islam,
                            Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin
                            J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao,
                            Miguel Martin, Effrosyni Mavroudi, Tushar
                            Nagarajan, Francesco Ragusa, Santhosh Kumar
                            Ramakrishnan, Luigi Seminara, Arjun Somayazulu,
                            Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu
                            Zhang, Angela Castillo, Changan Chen, Xinzhu Fu,
                            Ryosuke Furuta, Cristina Gonzalez, Prince Gupta,
                            Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo,
                            Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu,
                            Mi Luo, Zhengyi Luo, Brighid Meredith, Austin
                            Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny
                            Peng, Shraman Pramanick, Merey Ramazanova, Fiona
                            Ryan, Wei Shan, Kiran Somasundaram, Chenan Song,
                            Audrey Southerland, Masatoshi Tateno, Huiyu Wang,
                            Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang,
                            Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei
                            Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas
                            Bertasius, David Crandall, Dima Damen, Jakob Engel,
                            Giovanni Maria Farinella, Antonino Furnari, Bernard
                            Ghanem, Judy Hoffman, C. V. Jawahar, Richard
                            Newcombe, Hyun Soo Park, James M. Rehg, Yoichi
                            Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou,
                            <span>Michael Wray</span>
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2024</em>
                            <br>
                            <strong>
                                <a href="https://ego-exo4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2311.18259">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/climer_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Learning Temporal Sentence Grounding From Narrated EgoVideos</span>
                            <br>
                            Kevin Flanagan, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>BMVC, 2023</em>
                            <br>
                            <strong>
                                <a href="https://keflanagan.github.io/CliMer-TSG/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2310.17395">[arXiv]</a>
                                <a href="https://github.com/keflanagan/CliMer">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/contra_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">ConTra: (Con)text (Tra)nsformer for Cross-Modal Video Retrieval</span>
                            <br>
                            Adriano Fragomeni, <span>Michael Wray</span>, Dima
                            Damen,
                            <br>
                            <em>ACCV (Oral), 2022</em>
                            <br>
                            <strong>
                                <a href="https://adrianofragomeni.github.io/ConTra-Context-Transformer/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2210.04341">[arXiv]</a>
                                <a href="https://github.com/adrianofragomeni/ConTra">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/egovlp.jpeg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Egocentric Video-Language Pretraining</span>
                            <br>
                            Kevin Qinghong Lin, Alex Jinpeng Wang, Mattia
                            Soldan, <span>Michael Wray</span>, Rui Yan, Eric
                            Zhongcong Xu, Difei Gao, Rongcheng Tu, Wenzhe Zhao,
                            Weijie Kong, Chengfei Cai, Hongfa Wang, Dima Damen,
                            Bernard Ghanem, Wei Liu, Mike Zheng Shou
                            <br>
                            <em>NeurIPS, 2022</em>
                            <br>
                            <strong>
                                <a href="https://qinghonglin.github.io/EgoVLP/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2206.01670">[arXiv]</a>
                                <a href="https://github.com/showlab/EgoVLP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/ego4d.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego4D: Around the World in 3,000 Hours of Egocentric Video</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona Ryan, Jayant Sharma, <span>Michael Wray</span>, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR) 2022</em>
                            <br>
                            <strong>
                                <a href="https://ego4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2110.07058">[arXiv]</a>
                                <a href="https://ego4d-data.org/docs/">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/DACMR.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Domain Adaptation in Multi-View Embedding for Cross-Modal Video Retrieval</span>
                            <br>
                            Jonathan Munro, <span>Michael Wray</span>, Diane
                            Larlus, Gabriela Csurka, Dima Damen
                            <br>
                            <em>ArXiv, 2021</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2110.12812">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/epic_kitchens.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Rescaling Egocentric Vision</span>
                            <br>
                            Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and <span>Michael Wray</span>
                            <br>
                            <em>Springer International Journal of Computer Vision (IJCV), 2021</em>
                            <br>
                            <strong>
                                <a href="https://epic-kitchens.github.io/">[Webpage]</a>
                                <a href="https://github.com/epic-kitchens/epic-kitchens-100-annotations">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/ssvr_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">On Semantic Similarity in Video Retrieval</span>
                            <br>
                            <span>Michael Wray</span>, Hazel Doughty, and Dima Damen
                            <br>
                            <em>IEEE Conference on Computer Vision and Pattern Recognition, 2021.</em>
                            <br>
                            <strong>
                                <a href="https://mwray.github.io/SSVR/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2103.10095">[arXiv]</a>
                                <a href="https://github.com/mwray/Semantic-Video-Retrieval">[Code]</a>
                                <a href="https://mwray.github.io/SSVR/SSVR.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/sls.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Supervision Level Scales</span>
                            <br>
                            Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>arXiv, 2020</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2008.09890">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/fgar_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Fine-Grained Action Retrieval Through Multiple Parts-of-Speech Embeddings</span>
                            <br>
                            <span>Michael Wray</span>, Diane Larlus, Gabriela Csurka, and Dima Damen
                            <br>
                            <em>IEEE International Conference of Computer Vision, 2019</em>
                            <br>
                            <strong>
                                <a href="https://mwray.github.io/FGAR/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/1908.03477">[arXiv]</a>
                                <a href="https://github.com/mwray/Joint-Part-of-Speech-Embeddings">[Code]</a>
                                <a href="https://mwray.github.io/FGAR/FGAR.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/mvol_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Learning Visual Actions Using Multiple Verb-Only Labels</span>
                            <br>
                            <span>Michael Wray</span> and Dima Damen
                            <br>
                            <em>BMVA British Machine Vision Conference, 2019</em>
                            <br>
                            <strong>
                                <a href="https://mwray.github.io/MVOL/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/1907.11117">[arXiv]</a>
                                <a href="https://github.com/mwray/Multi-Verb-Labels">[Annotations]</a>
                                <a href="https://mwray.github.io/pdfs/MVOL.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/epic_kitchens.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</span>
                            <br>
                            Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and <span>Michael Wray</span>
                            <br>
                            <em>Springer European Conference on Computer Vision, 2018</em>
                            <br>
                            <strong>
                                <a href="https://epic-kitchens.github.io/">[Webpage]</a>
                                <a href="https://github.com/epic-kitchens/epic-kitchens-55-annotations">[Dataset]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/trbo_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Trespassing the Boundaries: Labeling Temporal Bounds For Object Interactions in Egocentric Video</span>
                            <br>
                            Davide Moltisanti, <span>Michael Wray</span>, Walterio Mayol-Cuevas, and Dima Damen
                            <br>
                            <em>IEEE International Conference on Computer Vision 2017</em>
                            <br>
                            <strong>
                                <a href="http://people.cs.bris.ac.uk/~damen/Trespass/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/1703.09026">[arXiv]</a>
                                <a href="https://mwray.github.io/pdfs/TrBo.pdf">[pdf]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/sembed_main_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">SEMBED: Semantic Embedding of Egocentric Action Videos</span>
                            <br>
                            <span>Michael Wray(*)</span>, Davide Moltisanti(*), Walterio Mayol-Cuevas, and Dima Damen
                            <br>
                            <em>Egocentric Perception, Interaction and Computing Workshop at ECCV 2016</em>
                            <br>
                            <strong>
                                <a href="http://people.cs.bris.ac.uk/~damen/SEMBED/">[Webpage]</a>
                                <a href="http://arxiv.org/abs/1607.08414">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                </tbody>
            </table>
            <!--end_research-->
            <em>To return home, click <a href="index.html">here</a>.</em>
            <hr>
        </div>
    </div>
</body>

<!--
                    <tr>
                        <td class=img>
                            <img src="img/<IMAGE_NAME>.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour"><NAME></span>
                            <br>
                            <span>Michael Wray</span>
                            <br>
                            <em><LOCATION></em>
                            <br>
                            <strong>
                                <a href="">[Webpage]</a>
                            </strong>
                        </td>
                    </tr>
-->
