<!DOCTYPE html>
<html lang="en">

  <head>

    <link rel="shortcut icon" type="image/ico" href="img/favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
      <a class="navbar-brand js-scroll-trigger" href="#page-top">
        <span class="d-block d-lg-none">Michael Wray</span>
        <span class="d-none d-lg-block">
          <img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="img/profile.jpg" alt="">
        </span>
      </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#about">About</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#news">News</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#research">Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#education">Education & Experience</a>
          </li>
          <li class="nav-item">
            <a class="nav-link js-scroll-trigger" href="#interests">Interests</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="pdfs/cv.pdf">CV</a>
          </li>
        </ul>
      </div>
    </nav>

    <div class="container-fluid p-0">

      <section class="resume-section p-3 p-lg-5 d-flex d-column" id="about">
        <div class="my-auto">
          <h1 class="mb-0">Michael
            <span class="text-primary">Wray</span>
          </h1>
          <div class="subheading mb-5">PhD Student . University of Bristol . 
            michael (dot) wray (at) bristol (dot) ac (dot) uk
          </div>
          <p class="mb-5">I am a final year PhD Student at the University of Bristol. My research focuses on action recognition and understanding its links with language.</p>
          <ul class="list-inline list-social-icons mb-0">
            <li class="list-inline-item">
              <a href="https://github.com/mwray">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>

            <li class="list-inline-item">
              <a href="pdfs/cv.pdf">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>

            <li class="list-inline-item">
              <a href="https://www.youtube.com/user/mwray100/featured">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          </ul>
        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="news">
        <div class="my-auto">
          <h2 class="mb-5">News</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper Accepted at ICCV </h3>
              <div class="subheading mb-3">Fine-Grained Action Retrieval through Multiple Parts of Speech Embeddings</div>
              <p>Our paper titled "Fine-Grained Action Retrieval through Multiple Parts of Speech Embeddings" 
              was accepted as a poster presentation at ICCV 2019 (27th Oct-2nd Nov 2019).
              pdf/video link to follow.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2019</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper Accepted at BMVC </h3>
              <div class="subheading mb-3">Learning Visual Actions Using Multiple Verb-Only Labels</div>
              <p>Our paper titled "Learning Visual Actions Using Multiple Verb-Only Labels" 
              was accepted as a poster presentation at BMVC 2019 (9th-12th September 2019).
              pdf/video link to follow.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2019</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Presentation at BMVA Symposium </h3>
              <div class="subheading mb-3">Presentation of Towards an Unequivocal Representation of Actions</div>
              <p>I presented a talk on Towards an Unequivocal
              Representation of Actions at BMVA Symposium: Robotics meets
              Semantics: Enabling Human-Level Understanding in Robots on 18th
              July. <a href="Towards.pptx">Slides.</a> <a href="https://www.youtube.com/watch?v=8rndQTQsEjE">Video Recording.</a></p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">EPIC Kitchens Demo at CVPR 2018</h3>
              <div class="subheading mb-3">Demo - Wednesday AM Booth 7</div>
              <p>Myself along with two other authors demoed EPIC at CVPR 2018.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Poster at BIVU2018</h3>
              <div class="subheading mb-3">Towards an Unequivocal Representation of Actions</div>
              <p>I presented a poster of Towards an Unequivocal Representation of Actions at the Brave New Ideas for Video Understanding workshop at CVPR2018.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">May 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">EPIC Kitchens</h3>
              <div class="subheading mb-3">Largest Egocentric Dataset</div>
              <p>We have just released the largest egocentric dataset for action and object recognition. More info can be found here.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">New Paper on ArXiv</h3>
              <div class="subheading mb-3">Towards an Unequivocal Representation of Actions</div>
              <p>We have released a shortform version of Towards an Unequivocal Representation of Actions on ArXiv here.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper accepted at ICCV 2017</h3>
              <div class="subheading mb-3">Trespassing the boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</div>
              <p>Davide Moltisanti presented the poster for the paper in ICCV2017.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2017</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Paper accepted at EPIC Workshop</h3>
              <div class="subheading mb-3">SEMBED: Semantic Embedding of Egocentric Action Videos</div>
              <p>Our paper SEMBED was presented at the first EPIC workshop during ECCV2016.</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2016</span>
            </div>
          </div>

        </div>

      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="research">
        <div class="my-auto">
          <h2 class="mb-5">Research</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings </h3>
              <div class="subheading mb-3"><!--<a href = "https://arxiv.org/abs/1804.02748">Arxiv</a></div>--!> Arxiv coming soon.</div>
              <p>We address the problem of cross-modal fine-grained action
                 retrieval between text and video. Cross-modal retrieval is commonly achieved
                 through learning a shared embedding space, that can indifferently embed
                 modalities. In this paper, we propose to enrich the embedding by disentangling
                 parts-of-speech (PoS) in the accompanying captions. We build a separate
                 multi-modal embedding space for each PoS tag. The outputs of multiple PoS
                 embeddings are then used as input to an integrated multi-modal space, where we
                 perform action retrieval </p>
            </div>

            <div class="modal fade" id="modalScaling" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings</h3>
                    <p>We address the problem of cross-modal fine-grained action
                     retrieval between text and video. Cross-modal retrieval is commonly achieved
                     through learning a shared embedding space, that can indifferently embed
                     modalities. In this paper, we propose to enrich the embedding by disentangling
                     parts-of-speech (PoS) in the accompanying captions. We build a separate
                     multi-modal embedding space for each PoS tag. The outputs of multiple PoS
                     embeddings are then used as input to an integrated multi-modal space, where we
                     perform action retrieval All embeddings are trained
                     jointly through a combination of PoS-aware and PoS-agnostic losses. Our
                     proposal enables learning specialised embedding spaces that offer multiple
                     views of the same embedded entities.</p>

                    <p>We report the first retrieval results on fine-grained
                     actions for the large-scale EPIC dataset, in a generalised zero-shot setting.
                     Results show the advantage of our approach for both video-to-text and
                     text-to-video action retrieval. We also demonstrate the benefit of
                     disentangling the PoS for the generic task of cross-modal video retrieval on
                     the MSR-VTT dataset.</p>

                     Overview of the approach:
                     <p><img src="fga/main_fig.png" class="img-fluid" alt="Results"/></p>

                    <h3 class="mb-0">Publication:</h3>
                    Fine-Grained Action Retrieval through Multiple Parts-of-Speech Embeddings.
                    Michael Wray, Diane Larlus, Gabriela Csurka and Dima Damen.
                    ICCV 2019.
                    </br>
                    <b>Link:</b> Coming Soon <!--<a href = "https://arxiv.org/abs/1804.02748">Arxiv</a>.--!>
                    

                    <h3 class="mb-0">Video:</h3>
                      <p>
                        <video width=100% controls>
                          <source src="fga/FGAR.mp4" type="video/mp4">
                          Your browser does not support HTML5 video.
                        </video>
                      </p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2019</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalScaling">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Learning Visual Actions Using Multiple Verb-Only Labels </h3>
              <div class="subheading mb-3"><!--<a href = "https://arxiv.org/abs/1804.02748">Arxiv</a></div>--!> Arxiv coming soon.</div>
              <p>This work introduces verb-only representations for both 
                 recognition and retrieval of visual actions, in video. Current 
                 methods neglect legitimate semantic ambiguities between verbs,
                 instead choosing unambiguous subsets of verbs along with 
                 objects to dis-ambiguate the actions. We instead propose 
                 multiple verb-only labels, which we learn through hard or soft 
                 assignment as a regression. </p>
            </div>

            <div class="modal fade" id="modalScaling" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Learning Visual Actions Using Multiple Verb-Only Labels</h3>
                    <p>This work introduces verb-only representations for both
                       recognition and retrieval of visual actions, in video. Current methods neglect
                       legitimate semantic ambiguities between verbs, instead choosing unambiguous
                       subsets of verbs along with objects to disambiguate the actions. We instead
                       propose multiple verb-only labels, which we learn through hard or soft
                       assignment as a regression. This enables learning a much larger vocabulary of
                       verbs, including contextual overlaps of these verbs.</p>

                    <p>We collect multi-verbannotations for three action video
                       datasets and evaluate the verb-only labelling represen-tations for action
                       recognition and cross-modal retrieval (video-to-text and text-to-video).We
                       demonstrate that multi-label verb-only representations outperform conventional
                       sin-gle verb labels.  We also explore other benefits of a multi-verb
                       representation includingcross-dataset retrieval and verb type (manner and
                       result verb types) retrieval.</p>

                    <h3 class="mb-0">Publication:</h3>
                    Learning Visual Actions Using Multiple Verb-Only Labels.
                    Michael Wray and Dima Damen.
                    BMVC 2019.
                    </br>
                    <b>Link:</b> Coming Soon <!--<a href = "https://arxiv.org/abs/1804.02748">Arxiv</a>.--!>
                    

                    <h3 class="mb-0">Video:</h3>
                      <p>
                        <video width=100% controls>
                          <source src="Learning/0615.mp4" type="video/mp4">
                          Your browser does not support HTML5 video.
                        </video>
                      </p>
                    <h3 class="mb-0">Annotations</h3>
                    <p> The annotations used in the paper can be found
                    <a href = "https://github.com/mwray/Multi-Verb-Labels">here.</a>
                    </p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">July 2019</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalScaling">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Towards an Unequivocal Representation of Actions</h3>
              <div class="subheading mb-3"><a href = "https://arxiv.org/abs/1805.04026">Arxiv</a></div>
              <p>This work introduces verb-only representations for actions and
              interactions; the problem of describing similar motions (e.g.
              'open door', 'open cupboard'), and distinguish differing ones
              (e.g. 'open door' vs 'open bottle') using verb-only labels.
              Current approaches for action recognition neglect legitimate
              semantic ambiguities and class overlaps between verbs, relying on
              the objects to disambiguate interactions.</p>

              <div class="modal fade" id="modalTowards" role="dialog">
                <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                  <div class="modal-content">
                    <div class="modal-body">
                      <h3 class="mb-0">Towards an Unequivocal Representation of Actions</h3>
                      <p>This work introduces verb-only representations for
                      actions and interactions; the problem of describing
                      similar motions (e.g.  'open door', 'open cupboard'), and
                      distinguish differing ones (e.g. 'open door' vs 'open
                      bottle') using verb-only labels.  Current approaches for
                      action recognition neglect legitimate semantic
                      ambiguities and class overlaps between verbs, relying on
                      the objects to disambiguate interactions.</p>

                      <p> We deviate from single-verb labels and introduce a
                      mapping between observations and multiple verb labels -
                      in order to create an Unequivocal Representation of
                      Actions. The new representation benefits from increased
                      vocabulary and a soft assignment to an enriched space of
                      verb labels.  </p>
                  
                      <p> We learn these representations as multi-output
                      regression, using a two-stream fusion CNN. The proposed
                      approach outperforms conventional single-verb labels
                      (also known as majority voting) on three egocentric
                      datasets for both recognition and retrieval.  </p>

                      <h3 class="mb-0">Publication:</h3>
                      Towards an Unequivocal Representation of Actions, Michael Wray, Davide Moltisanti and Dima Damen (2018).
                      <b>Link:</b> <a href = "https://arxiv.org/abs/1805.04026">Arxiv</a>.
                      

                      <h3 class="mb-0">Video:</h3>
                      <p>
                        <video width=100% controls>
                          <source src="Towards/towards.mp4" type="video/mp4">
                          Your browser does not support HTML5 video.
                        </video>
                      </p>

                      <h3 class="mb-0">Presentation:</h3>
                      <p><iframe width="560" height="315" src="https://www.youtube.com/embed/8rndQTQsEjE" frameborder="0" allowfullscreen></iframe></p>
                      <h3 class="mb-0">Results</h3>
                      Example cross dataset retrievals using a soft assigned multi label approach.
                      <p><img src="Towards/results.png" class="img-fluid" alt="Results" width=900/></p>

                    </div>
                    <div class="modal-footer">
                      <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                    </div>
                  </div>
                  
                </div>
              </div>
                </div>
                <div class="resume-date text-md-right">
                  <span class="text-primary">May 2018</span>
                  </br>
                  <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalTowards">More Info</button>
                </div>
              </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
              <div class="subheading mb-3"><a href = "https://arxiv.org/abs/1804.02748">Arxiv</a></div>
              <p>First-person vision is gaining interest as it offers a unique
              viewpoint on people's interaction with objects, their attention,
              and even intention. However, progress in this challenging domain
              has been relatively slow due to the lack of sufficiently large
              datasets. In this paper, we introduce EPIC-KITCHENS, a
              large-scale egocentric video benchmark recorded by 32
              participants in their native kitchen environments. Our videos
              depict nonscripted daily activities: we simply asked each
              participant to start recording every time they entered their
              kitchen</p>
            </div>

            <div class="modal fade" id="modalScaling" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Scaling Egocentric Vision: The EPIC-KITCHENS Dataset</h3>
                    <p>The largest dataset in first-person (egocentric)
                    vision; multi-faceted non-scripted recordings in native
                    environments - i.e. the wearers' homes, capturing all
                    daily activities in the kitchen over multiple days.
                    Annotations are collected using a novel `live' audio
                    commentary approach.</p>

                    <h3 class="mb-0">Publication:</h3>
                    Scaling Egocentric Vision: The EPIC-KITCHENS Dataset.
                    Dima Damen, Hazel Doughty, Giovanni Maria Farinella,
                    Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide
                    Moltisanti, Jonathan Munro, Toby Perrett, Will Price,
                    Michael Wray (2018)
                    </br>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1804.02748">Arxiv</a>.
                    

                    <h3 class="mb-0">Video:</h3>
                    <p><iframe width="560" height="315" src="https://www.youtube.com/embed/Dj6Y3H0ubDw" frameborder="0" allowfullscreen></iframe></p>

                    <h3 class="mb-0">Website</h3>
                    <p> The official website of EPIC-Kitchens can be found 
                    <a href = "https://epic-kitchens.github.io/2018">here.</a>
                    This includes links to download the dataset and
                    annotations.</p>

                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2018</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalScaling">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</h3>
              <div class="subheading mb-3"><a href = "http://openaccess.thecvf.com/content_ICCV_2017/papers/Moltisanti_Trespassing_the_Boundaries_ICCV_2017_paper.pdf">ICCV 2017</a></div>
              <p>Manual annotations of temporal bounds for object interactions
              (i.e. start and end times) are typical training input to
              recognition, localization and detection algorithms. For three
              publicly available egocentric datasets, we uncover
              inconsistencies in ground truth temporal bounds within and across
              annotators and datasets. We systematically assess the robustness
              of state-of-the-art approaches to changes in labeled temporal
              bounds, for object interaction recognition.</p>
            </div>

            <div class="modal fade" id="modalTrespassing" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">Trespassing the Boundaries: Labeling Temporal Bounds for Object Interactions in Egocentric Video</h3>
                    <p>Manual annotations of temporal bounds for object
                    interactions (i.e. start and end times) are typical
                    training input to recognition, localization and detection
                    algorithms. For three publicly available egocentric
                    datasets, we uncover inconsistencies in ground truth
                    temporal bounds within and across annotators and datasets.
                    We systematically assess the robustness of state-of-the-art
                    approaches to changes in labeled temporal bounds, for
                    object interaction recognition.</p>

                    <h3 class="mb-0">Publication:</h3>
                    Trespassing the Boundaries: Labeling Temporal Bounds for
                    Object Interactions in Egocentric Video. Davide Moltisanti,
                    Michael Wray and Dima Damen(2017).
                    </br>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1703.09026">Arxiv</a>.
                    
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">October 2017</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalTrespassing">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">sembed: semantic embedding of egocentric action videos</h3>
              <div class="subheading mb-3">ECCVW 2016 - <a href= "./pdfs/ECCVW2016SEMBED.pdf">pdf</a></div>
              <p>we present sembed, an approach for embedding an egocentric
              object interaction video in a semantic-visual graph to estimate
              the probability distribution over its potential semantic
              labels.</p>
            </div>

            <div class="modal fade" id="modalSembed" role="dialog">
              <div class="modal-dialog modal-dialog-centered modal-lg" role="document">
                <div class="modal-content">
                  <div class="modal-body">
                    <h3 class="mb-0">SEMBED: Semantic Embedding of Egocentric Action Videos</h3>
                    <p> Egocentric Action Recognition has largely been
                    performed on datasets where annotators had a choice from a
                    finite set of semantically distinct verbs.  </p>
                    
                    <p> When allowed free choice over both the verb chosen to
                    describe an action and the temporal boundaries the
                    resulting dataset contains a wide variety of different
                    labels. Whilst these labels are diverse we treat them all
                    as correct labels. Standard one vs. all classifying
                    techniques, such as SVM, are unable to deal with the
                    ambiguities introduced by the free annotations and so a
                    graphical approach is introduced.  </p>
                    
                    <p> We present SEMBED, a method which is capable of dealing
                    with the ambiguity within the dataset by embedding videos
                    in a Semantic Visual Graph. Multiple state-of-the-art
                    features have been tested (in the form of Improved Dense
                    Trajectories and Overfeat CNN) along with Bag of (Visual)
                    Words and Fisher Vectors as encoding methods. We find that
                    SEMBED using the notion of embedding videos in a graph that
                    are linked visually and/or semantically is able to beat SVM
                    by more than 5%.  </p>

                    <h3 class="mb-0">Publication:</h3>
                    SEMBED: Semantic Embedding of Egocentric Action Videos.
                    Michael Wray(*), Davide Moltisanti(*), Walterio
                    Mayol-Cuevas and Dima Damen(2016). <i> In Egocentric 
                    Perception, Interaction and Computing Workshop ECCV 2016, 
                    Amsterdam, The Netherlands.</i>
                    <p><i>* Denotes equal contribution</i></p>
                    <b>Link:</b> <a href = "https://arxiv.org/abs/1607.08414">Arxiv</a>.

                    <h3 class="mb-0">Video:</h3>
                    <p><iframe width="560" height="315" src="https://www.youtube.com/embed/6bDDTIJUuic" frameborder="0" allowfullscreen></iframe></p>

                    <h3>Dataset:</h3>
                    <a href="http://www.cs.bris.ac.uk/~Damen/BEOID/">Bristol Egocentric Object Interactions Dataset</a>
                  </div>
                  <div class="modal-footer">
                    <button type="button" class="btn btn-default" data-dismiss="modal">Close</button>
                  </div>
                </div>
                
              </div>
            </div>

            <div class="resume-date text-md-right">
              <span class="text-primary">october 2016</span>
              </br>
              <button type="button" class="btn btn-info btn-lg" data-toggle="modal" data-target="#modalSembed">More Info</button>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">The Cage: Towards a 6-DoF Remote Control with Force Feedback for UAV Interaction.</h3>
              <div class="subheading mb-3">Extended Abstract CHI 2015</div>
              <p>Unmanned Aerial Vehicles (UAVs) require complex control and
              significant experience for piloting. While these devices continue
              to improve, there is, as yet, no device that affords six degrees
              of freedom (6-DoF) control and directional haptic feedback. We
              present The Cage, a 6-DoF controller for piloting an unmanned
              aerial vehicle (UAV).</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">April 2015</span>
            </div>
          </div>

        </div>
      </section>

      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="education">
        <div class="my-auto">
          <h2 class="mb-5">Education and Experience</h2>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">University of Bristol</h3>
              <div class="subheading mb-3">PhD in Computer Vision</div>
              <p>Ongoing</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">September 2015 - Current</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Naver Labs Europe</h3>
              <div class="subheading mb-3">Research Internship</div>
              <p>Supervised by Gabriela Csurka and Diane Larlus</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">Autumn 2017</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">Cisco</h3>
              <div class="subheading mb-3">Internship</div>
              <div>Router Testing/Development</div>
              <p>3 Month Internship</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">June 2014 - August 2014</span>
            </div>
          </div>

          <div class="resume-item d-flex flex-column flex-md-row mb-5">
            <div class="resume-content mr-auto">
              <h3 class="mb-0">University of Bristol</h3>
              <div class="subheading mb-3">Master of Engineering</div>
              <div>Computer Science</div>
              <p>First Class</p>
            </div>
            <div class="resume-date text-md-right">
              <span class="text-primary">September 2011 - May 2015</span>
            </div>
          </div>


        </div>
      </section>


      <section class="resume-section p-3 p-lg-5 d-flex flex-column" id="interests">
        <div class="my-auto">
          <h2 class="mb-5">Interests</h2>
          <p>Whilst not working on completing my PhD I enjoy reading - primarily Science Fiction and Fantasy. Below are few books/series I would recommend:</p>
          <ul>
              <li>Wheel of Time - Robert Jordan/Brandon Sanderson.</li>
              <li>Lightbringer - Brent Weeks.</li>
              <li>Rendezvous with Rama - Arthur C. Clarke.</li>
          </ul>
          <p class="mb-0"></p>
        </div>
      </section>

    </div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/website.min.js"></script>

  </body>

</html>
