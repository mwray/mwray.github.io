<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Michael Wray</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <div id="mainBar">
        <div id="textBar">
            <hr>
            <header>
                <h1>
                    <img class="profiile-img" src="img/michael-w_hs_2020.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
                </h1>
                <h1> Michael Wray</h1>
                <h2> Assistant Professor of Computer Vision</h2>
            </header>

                <p>I am a lecturer/Assistant Professor of Computer Vision at
                the School of Computer Science at the University of
                Bristol. My research interests are in multi-modal video
                understanding, particularly for egocentric videos — focusing on
                how both vision and language can be tied together towards tasks
                such as cross-modal retrieval, grounding and captioning.
                I am part of <a href="https://uob-mavi.github.io/">MaVi</a> and
                <a href="https://vilab.blogs.bristol.ac.uk/">ViLab</a>.
                </p>

            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="https://github.com/mwray">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://www.youtube.com/user/mwray100/featured">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-youtube-play fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=gFQcKZMAAAAJ&hl=en&oi=ao">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>

                <li class="list-inline-item">
                    <a href="https://twitter.com/mwray0">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter-square fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>

            <h4> Email: michael (dot) wray (at) bristol (dot) ac (dot) uk </h4>

            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">October 2024</span> - <b>Paper Accepted at WACV 2025</b> Our paper titled Moment of Untruth: Dealing with Negative Queries in Video Moment Retrieval wass accepted at WACV 2025. More details coming soon!
                <li> <span class="prim-colour">September 2024</span> - <b>Area Chair at CVPR 2025</b> Honoured to be an Area Chair for CVPR 2025.
                <li> <span class="prim-colour">August 2024</span> - <b>UCL Workshop Talk</b> I gave a talk on Egocentric Video Understanding at Advancements in Time Series Analysis for Computer Vision.
                <li> <span class="prim-colour">August 2024</span> - <b>New Paper at ICPR 2024</b> Our paper SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition was accepted at ICPR 2024. Read it <a href="https://arxiv.org/abs/2408.10037">here</a>.
                <li> <span class="prim-colour">August 2024</span> - <b>New Paper at ACVR Workshop 2024</b> Our paper Video Editing for Video Retrieval was accepted at the ACVR workshop at ECCV 2024. Read it <a href="https://arxiv.org/abs/2402.02335">here</a>.
                <li> <span class="prim-colour">July 2024</span> - <b>Associate Editor for IET Computer Vision</b> Honoured to be an associate editor for IET in Computer Vision.
                <li> <span class="prim-colour">July 2024</span> - <b>BMVA Summer School Lecture</b> I gave a lecture on Egocentric vision at the BMVA summer school in Durham, link to slides <a href="https://uob-my.sharepoint.com/:p:/r/personal/mw1760_bristol_ac_uk/Documents/presentations/bmva_ss/Egocentric_bmva_ss_2024.pptx?d=w5d0de05b322b40d0942c3bded37f2494&csf=1&web=1&e=Jy9HHs">here</a>.
                <li> <span class="prim-colour">June 2024</span> - <b>Internal Examiner for 2 Theses</b> Honoured to act as an internal examiner for 2 students: Ruixiong Wang and Mengjie Zhou. Congratulations to both for passing with minor Corrections!
</ul>
            <!--end_news-->

            <p>
                <em>For a full list of News, click <a href="news.html">here</a>.</em>
            <p>

            <hr>

            <h3>Research</h3>

            <br>
            <p>
                <em>Short list of Research projects, click <a href="research.html">here</a> for a full list.</em>
            <p>


            <table class="researchtable">
                <tbody>
            <!--research_section-->
                        <td class=img>
                            <img src="img/SHARP_qualitative.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">SHARP: Segmentation of Hands and Arms by Range using Pseudo-Depth for Enhanced Egocentric 3D Hand Pose Estimation and Action Recognition</span>
                            <br>
                            Wiktor Mucha, <span>Michael Wray</span>, Martin Kampel
                            <br>
                            <em>ICPR, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2408.10037">[arXiv]</a>
                                <a href="https://github.com/wiktormucha/SHARP">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/hoi-ref_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HOI-Ref: Hand-Object Interaction Referral in Egocentric Vision</span>
                            <br>
                            Siddhant Bansal, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>arXiv, 2024</em>
                            <br>
                            <strong>
                                <a href="https://sid2697.github.io/hoi-ref/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2404.09933">[arXiv]</a>
                                <a href="https://github.com/Sid2697/HOI-Ref">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/video_edit_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Video Editing for Video Retrieval</span>
                            <br>
                            Bin Zhu, Kevin Flanagan, Adriano Fragomeni, <span>Michael Wray</span>, Dima Damen
                            <br>
                            <em>Twelfth International Workshop on Assistive Computer Vision and Robotics ECCV, 2024</em>
                            <br>
                            <strong>
                                <a href="https://arxiv.org/abs/2402.02335">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/genhowto_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">GenHowTo: Learning to Generate Actions and State Transformations from Instructional Videos</span>
                            <br>
                            Tomáš Souček, Dima Damen, <span>Michael Wray</span>, Ivan Laptev, Josef Sivic
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2024</em>
                            <br>
                            <strong>
                                <a href="https://soczech.github.io/genhowto/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2312.07322">[arXiv]</a>
                                <a href="https://github.com/soCzech/genhowto">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/ego-exo4d_intro_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives</span>
                            <br>
                            Kristen Grauman, Andrew Westbury, Lorenzo
                            Torresani, Kris Kitani, Jitendra Malik,
                            Triantafyllos Afouras, Kumar Ashutosh, Vijay
                            Baiyya, Siddhant Bansal, Bikram Boote, Eugene
                            Byrne, Zach Chavis, Joya Chen, Feng Cheng, Fu-Jen
                            Chu, Sean Crane, Avijit Dasgupta, Jing Dong, Maria
                            Escobar, Cristhian Forigua, Abrham Gebreselasie,
                            Sanjay Haresh, Jing Huang, Md Mohaiminul Islam,
                            Suyog Jain, Rawal Khirodkar, Devansh Kukreja, Kevin
                            J Liang, Jia-Wei Liu, Sagnik Majumder, Yongsen Mao,
                            Miguel Martin, Effrosyni Mavroudi, Tushar
                            Nagarajan, Francesco Ragusa, Santhosh Kumar
                            Ramakrishnan, Luigi Seminara, Arjun Somayazulu,
                            Yale Song, Shan Su, Zihui Xue, Edward Zhang, Jinxu
                            Zhang, Angela Castillo, Changan Chen, Xinzhu Fu,
                            Ryosuke Furuta, Cristina Gonzalez, Prince Gupta,
                            Jiabo Hu, Yifei Huang, Yiming Huang, Weslie Khoo,
                            Anush Kumar, Robert Kuo, Sach Lakhavani, Miao Liu,
                            Mi Luo, Zhengyi Luo, Brighid Meredith, Austin
                            Miller, Oluwatumininu Oguntola, Xiaqing Pan, Penny
                            Peng, Shraman Pramanick, Merey Ramazanova, Fiona
                            Ryan, Wei Shan, Kiran Somasundaram, Chenan Song,
                            Audrey Southerland, Masatoshi Tateno, Huiyu Wang,
                            Yuchen Wang, Takuma Yagi, Mingfei Yan, Xitong Yang,
                            Zecheng Yu, Shengxin Cindy Zha, Chen Zhao, Ziwei
                            Zhao, Zhifan Zhu, Jeff Zhuo, Pablo Arbelaez, Gedas
                            Bertasius, David Crandall, Dima Damen, Jakob Engel,
                            Giovanni Maria Farinella, Antonino Furnari, Bernard
                            Ghanem, Judy Hoffman, C. V. Jawahar, Richard
                            Newcombe, Hyun Soo Park, James M. Rehg, Yoichi
                            Sato, Manolis Savva, Jianbo Shi, Mike Zheng Shou,
                            <span>Michael Wray</span>
                            <br>
                            <em>IEEE/CVF Computer Vision and Pattern Recognition (CVPR), 2024</em>
                            <br>
                            <strong>
                                <a href="https://ego-exo4d-data.org/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2311.18259">[arXiv]</a>
                            </strong>
                        </td>
                    </tr>
                        <td class=img>
                            <img src="img/climer_fig.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Learning Temporal Sentence Grounding From Narrated EgoVideos</span>
                            <br>
                            Kevin Flanagan, Dima Damen, <span>Michael Wray</span>
                            <br>
                            <em>BMVC, 2023</em>
                            <br>
                            <strong>
                                <a href="https://keflanagan.github.io/CliMer-TSG/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2310.17395">[arXiv]</a>
                                <a href="https://github.com/keflanagan/CliMer">[Code]</a>
                            </strong>
                        </td>
                    </tr>
</tbody>
</table>
            <!--end_research-->
                </tbody>
            </table>

            <br>
            <p>
                <em>For a full list of Research projects, click <a href="research.html">here</a>.</em>
            <p>

            <hr>
            
            <h3>Short Bio</h3>

            Michael is a lecturer in Computer Vision at the School of Computer
            Science at the University of Bristol. He finished his PhD titled
            "Verbs and Me: an Investigation into Verbs as Labels for Action
            Recognition in Video Understanding" in 2019 under the supervision
            of Professor Dima Damen. After, he stayed in the same lab as a
            Post-Doc working on Vision and Language and the collection of the
            Ego4D Dataset. Michael has led the organisation EPIC workshop
            series from 2021 onwards, is an organiser of the Ego4D workshop
            series, and is an ELLIS member.

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li><span class="prim-colour">Applied Deep Learning</span> 22/23, 23/24. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                <li><span class="prim-colour">Computer Systems A</span> 22/23. 23/24. <a href="https://uob-csa.github.io/website/">Webpage</a>.
                <li><span class="prim-colour">Individual Projects</span> 22/23. 23/24. <a href="https://cs-uob-individual-project.github.io/">Webpage</a>.
            </ul>

            <hr>

            <h3>People</h3>

            <h4>Current</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour"><a href="https://adrianofragomeni.github.io/">Adriano Fragomeni</a></span>: PhD, 2020&#8211;Current (w/ Dima Damen)</li>

                <li><span class="prim-colour"><a href="http://www.bristol.ac.uk/cdt/interactive-ai/current-students/2020-cohort/flanagan/">Kevin Flanagan</a></span>: PhD, 2021&#8211;Current (w/ Dima Damen)</li>
                <li><span class="prim-colour">Shijia Feng</a></span>: PhD, 2022&#8211;Current (w/ Walterio Mayol Cuevas)</li>
                <li><span class="prim-colour">Sam Pollard</a></span>: MEng, PhD, 2023&#8211;Current</li>
                <li><span class="prim-colour">Beth Pearson</a></span>: PhD, 2023&#8211;Current (w/ Martha Lewis)</li>
                <li><span class="prim-colour">Fahd Abdelazim</a></span>: PhD, 2024&#8211;Current </li>
            </ul>

            <h4>Previous</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">Benjamin Gutierrez Serafin</span>: MSc, 2020</li>
                <li><span class="prim-colour">Pei Huang</span>: MSc, 2016</li>
                <li><span class="prim-colour">Lee Tancock</a></span>: MEng, 2023</li>
                <li><span class="prim-colour">Zac Woodford</a></span>: MEng, 2023</li>
                <li><span class="prim-colour">Richa Banthia</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Alex Elwood</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Moise Guran</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Rahat Mittal</a></span>: MEng, 2024</li>
                <li><span class="prim-colour">Bence Szarka</a></span>: MEng, 2024</li>
            </ul>

            <hr>

            <h3>Misc.</h3>

            <h4>Workshop Organiser</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">WINVU</span>: <a href="">CVPR2024</a></li>
                <li><span class="prim-colour">EPIC@</span>: <a href="https://www.eyewear-computing.org/EPIC_ICCV21/">ICCV2021</a>, <a href="https://www.eyewear-computing.org/EPIC_CVPR21/">CVPR2021</a>, <a href="https://www.eyewear-computing.org/EPIC_ECCV20/">ECCV2020</a></li>
                <li><span class="prim-colour">Joint Ego4D+EPIC@</span>: <a href="https://sites.google.com/view/ego4d-epic-cvpr2023-workshop/">CVPR2023</a>, <a href="https://sites.google.com/view/cvpr2022w-ego4d-epic/">CVPR2022</a></li>
                <li><span class="prim-colour">Ego4D@</span>: <a href="https://ego4d-data.org/workshops/eccv22/">ECCV2022</a></li>
            </ul>

            <h4>Area Chair</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2025, 2024, 2023</li>
                <li><span class="prim-colour">ECCV: </span>2024</li>
                <li><span class="prim-colour">NeurIPS: </span>2024</li>
            </ul>

            <h4>Associate Editor</h4>
            <ul class="no-bullets">
                <li> <span class="prim-colour">IET Computer Vision</span> 2024&#8211;Current </li>
                <li> <span class="prim-colour">ToMM Special Issue on Text-Multimedia Retrieval </span> 2024 </li>
            </ul>

            <h4>Outstanding Reviewer</h4>

            <ul class="no-bullets">
                <li><span class="prim-colour">BMVC2024</span></li>
                <li><span class="prim-colour">ECCV2022</span></li>
                <li><span class="prim-colour">ICCV2021</span></li>
                <li><span class="prim-colour">CVPR2021</span></li>
                <li><span class="prim-colour">BMVC2020</span></li>
            </ul>


            <h4> Reviewing Duties</h4>

            <h5> Conferences</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">CVPR: </span>2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">NeurIPS: </span>2023</li>
                <li><span class="prim-colour">NeurIPS D&B Track: </span>2024, 2023</li>
                <li><span class="prim-colour">ICCV: </span>2023, 2021</li>
                <li><span class="prim-colour">ECCV: </span>2022</li>
                <li><span class="prim-colour">ACCV: </span>2024, 2022, 2020</li>
                <li><span class="prim-colour">BMVC: </span>2024, 2023, 2022, 2021, 2020, 2019</li>
                <li><span class="prim-colour">WACV: </span>2024, 2023, 2022, 2021</li>
            </ul>
            
            <h5> Journals</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">TPAMI</span></li>
                <li><span class="prim-colour">IJCV</span></li>
                <li><span class="prim-colour">TCSVT</span></li>
                <li><span class="prim-colour">Pattern Recognition</span></li>
                <li><span class="prim-colour">TOMM</span></li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>

